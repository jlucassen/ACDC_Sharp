{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6606875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import einops\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from EAP_wrapper import EAP\n",
    "\n",
    "from jaxtyping import Float\n",
    "\n",
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a16eab",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20df2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dfbf6",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601a7d92",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/james/Documents/My Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m25\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m clean_dataset \u001b[39m=\u001b[39m IOIDataset(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     prompt_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmixed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     N\u001b[39m=\u001b[39mN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     device\u001b[39m=\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m corr_dataset \u001b[39m=\u001b[39m clean_dataset\u001b[39m.\u001b[39;49mgen_flipped_prompts(\u001b[39m'\u001b[39;49m\u001b[39mABC->XYZ, BAB->XYZ\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m make_table(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   colnames \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mIOI prompt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mIOI subj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mIOI indirect obj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mABC prompt\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   cols \u001b[39m=\u001b[39m [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   title \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSentences from IOI vs ABC distribution\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/james/Documents/My%20Documents/Projects/ACDC_Sharp/attr_patching/eap_ioi_demo.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/My Documents/Projects/ACDC_Sharp/attr_patching/ioi_dataset.py:705\u001b[0m, in \u001b[0;36mIOIDataset.gen_flipped_prompts\u001b[0;34m(self, flip)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Get flipped prompts\u001b[39;00m\n\u001b[1;32m    703\u001b[0m flipped_prompts \u001b[39m=\u001b[39m gen_flipped_prompts(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mioi_prompts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplates_by_prompt, flip, NAMES, seed)\n\u001b[0;32m--> 705\u001b[0m flipped_ioi_dataset \u001b[39m=\u001b[39m IOIDataset(\n\u001b[1;32m    706\u001b[0m     prompt_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt_type,\n\u001b[1;32m    707\u001b[0m     N\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mN,\n\u001b[1;32m    708\u001b[0m     tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    709\u001b[0m     prompts\u001b[39m=\u001b[39;49mflipped_prompts,\n\u001b[1;32m    710\u001b[0m     prefixes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprefixes,\n\u001b[1;32m    711\u001b[0m     prepend_bos\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepend_bos,\n\u001b[1;32m    712\u001b[0m     manual_word_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_idx,\n\u001b[1;32m    713\u001b[0m     has_been_flipped\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    714\u001b[0m     seed\u001b[39m=\u001b[39;49mseed\n\u001b[1;32m    715\u001b[0m )\n\u001b[1;32m    716\u001b[0m \u001b[39mreturn\u001b[39;00m flipped_ioi_dataset\n",
      "File \u001b[0;32m~/Documents/My Documents/Projects/ACDC_Sharp/attr_patching/ioi_dataset.py:692\u001b[0m, in \u001b[0;36mIOIDataset.__init__\u001b[0;34m(self, prompt_type, N, tokenizer, prompts, symmetric, prefixes, nb_templates, prepend_bos, manual_word_idx, has_been_flipped, seed, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenized_prompts\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    688\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(tok) \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoks[i]])\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m--> 692\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[0;32m~/Documents/My Documents/Projects/ACDC_Sharp/attr_patching/ioi_dataset.py:753\u001b[0m, in \u001b[0;36mIOIDataset.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device):\n\u001b[0;32m--> 753\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoks\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ACDC/lib/python3.12/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from ioi_dataset import IOIDataset, format_prompt, make_table\n",
    "N = 25\n",
    "clean_dataset = IOIDataset(\n",
    "    prompt_type='mixed',\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=device\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')\n",
    "\n",
    "make_table(\n",
    "  colnames = [\"IOI prompt\", \"IOI subj\", \"IOI indirect obj\", \"ABC prompt\"],\n",
    "  cols = [\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "    model.to_string(clean_dataset.s_tokenIDs).split(),\n",
    "    model.to_string(clean_dataset.io_tokenIDs).split(),\n",
    "    map(format_prompt, clean_dataset.sentences),\n",
    "  ],\n",
    "  title = \"Sentences from IOI vs ABC distribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657f126",
   "metadata": {},
   "source": [
    "# Metric Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: 2.805164098739624, Corrupt direction: 1.6204073429107666\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_diff(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    ioi_dataset: IOIDataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    # Get logits for indirect objects\n",
    "    batch_size = logits.size(0)\n",
    "    io_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.io_tokenIDs[:batch_size]]\n",
    "    s_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.s_tokenIDs[:batch_size]]\n",
    "    # Get logits for subject\n",
    "    logit_diff = io_logits - s_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "with t.no_grad():\n",
    "    clean_logits = model(clean_dataset.toks)\n",
    "    corrupt_logits = model(corr_dataset.toks)\n",
    "    clean_logit_diff = ave_logit_diff(clean_logits, clean_dataset).item()\n",
    "    corrupt_logit_diff = ave_logit_diff(corrupt_logits, corr_dataset).item()\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    ioi_dataset: IOIDataset = clean_dataset\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(logits, ioi_dataset)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "def negative_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -ioi_metric(logits)\n",
    "    \n",
    "# Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    clean_metric = ioi_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, clean_dataset)\n",
    "    corrupt_metric = ioi_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, corr_dataset)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81ab6e",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving activations requires 0.0004 GB of memory per token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.9.9 -> [-0.027] -> head.11.10.q\n",
      "head.10.7 -> [0.027] -> head.11.10.q\n",
      "head.5.5 -> [0.02] -> head.8.6.v\n",
      "mlp.0 -> [0.02] -> head.6.9.q\n",
      "head.5.5 -> [-0.019] -> mlp.5\n",
      "head.9.9 -> [-0.018] -> head.10.7.q\n",
      "mlp.0 -> [-0.017] -> mlp.4\n",
      "mlp.0 -> [-0.016] -> head.11.10.k\n",
      "head.5.5 -> [-0.014] -> head.6.9.q\n",
      "head.3.0 -> [-0.013] -> mlp.5\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "\n",
    "graph = EAP(\n",
    "    model,\n",
    "    clean_dataset.toks,\n",
    "    corr_dataset.toks,\n",
    "    ioi_metric,\n",
    "    upstream_nodes=[\"mlp\", \"head\"],\n",
    "    downstream_nodes=[\"mlp\", \"head\"],\n",
    "    batch_size=25\n",
    ")\n",
    "\n",
    "top_edges = graph.top_edges(n=10, abs_scores=True)\n",
    "for from_edge, to_edge, score in top_edges:\n",
    "    print(f'{from_edge} -> [{round(score, 3)}] -> {to_edge}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
